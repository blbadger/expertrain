python merge_adapters.py \
--model_name_or_path "/home/bbadger/Desktop/llama-3.2-3b-instruct" \
--lora_weights_path "/home/bbadger/experiments/llama3.2-3b-bird-lora" \
--use_peft_lora True \
--lora_r 64 \
--lora_alpha 64 \
--lora_dropout 0. \
--lora_target_modules "all-linear" \
--use_4bit_quantization False \
